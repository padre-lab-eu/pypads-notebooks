{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_ACCESS_KEY_ID=XSHDLTACETFLQURZSAUJ\n",
    "%env AWS_SECRET_ACCESS_KEY=ISvyN9Aay40ZIaMuQcngOCJdzkSqR85ON1ng9PNZ\n",
    "%env MLFLOW_S3_ENDPOINT_URL=http://s3.padre-lab.eu\n",
    "%env MLFLOW_TRACKING_URI=http://mlflow.padre-lab.eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-05 14:36:51.002 | INFO     | pypads.app.base:activate_tracking:507 - Activating tracking...\n",
      "2020-10-05 14:37:00.885 | INFO     | pypads.injections.setup.misc_setup:_call:72 - Tracking execution to run with id 3f37b1482453464b8cab677047b43c5d\n",
      "2020-10-05 14:37:02.055 | WARNING  | pypads.app.misc.managed_git:preserve_changes:84 - There are uncommitted changes in your git!\n",
      "2020-10-05 14:37:02.146 | INFO     | pypads.app.misc.managed_git:preserve_changes:96 - Using already existing pypads branch PyPads/f1ab4cea22ef4d45b03922e58fdb2d95\n",
      "2020-10-05 14:37:02.361 | WARNING  | pypads.app.misc.managed_git:_handle_error:18 - Couldn't initialized git repository because of exception: Preserving commit failed due to INVALID_PARAMETER_VALUE: Tag value 'diff --git a/.idea/workspace.xml b/.idea/workspace.xml\n",
      "new file mode 100644\n",
      "index 0000000..08385ea\n",
      "--- /dev/null\n",
      "+++ b/.idea/workspace.xml\n",
      "@@ -0,0 +1,229 @@\n",
      "+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "+<project version=\"4\">\n",
      "+  <component name=\"BranchesTr' had length 24822, which exceeded length limit of 5000\n",
      "2020-10-05 14:37:02.794 | WARNING  | pypads.app.base:start_track:620 - Active experiment_id of run doesn't match given input name PyTorch Example. Recreating new run.\n",
      "2020-10-05 14:37:11.557 | INFO     | pypads.injections.setup.misc_setup:_call:72 - Tracking execution to run with id 5cca87cdda4d4193880c75ce3363f279\n",
      "2020-10-05 14:37:12.622 | WARNING  | pypads.app.misc.managed_git:preserve_changes:84 - There are uncommitted changes in your git!\n",
      "2020-10-05 14:37:12.770 | INFO     | pypads.app.misc.managed_git:preserve_changes:96 - Using already existing pypads branch PyPads/f1ab4cea22ef4d45b03922e58fdb2d95\n",
      "2020-10-05 14:37:13.029 | WARNING  | pypads.app.misc.managed_git:_handle_error:18 - Couldn't initialized git repository because of exception: Preserving commit failed due to INVALID_PARAMETER_VALUE: Tag value 'diff --git a/.idea/workspace.xml b/.idea/workspace.xml\n",
      "new file mode 100644\n",
      "index 0000000..08385ea\n",
      "--- /dev/null\n",
      "+++ b/.idea/workspace.xml\n",
      "@@ -0,0 +1,229 @@\n",
      "+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "+<project version=\"4\">\n",
      "+  <component name=\"BranchesTr' had length 24822, which exceeded length limit of 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pypads.app.base.PyPads at 0x7f64588c6ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypads.app.base import PyPads\n",
    "\n",
    "tracker = PyPads(uri=\"http://mlflow.padre-lab.eu\")\n",
    "tracker.activate_tracking()\n",
    "tracker.start_track(experiment_name=\"PyTorch Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christofer/PycharmProjects/pypads-notebooks/venv/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/christofer/PycharmProjects/pypads-notebooks/venv/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    }
   ],
   "source": [
    "tracker.actuators.set_random_seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christofer/PycharmProjects/pypads-notebooks/venv/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christofer/PycharmProjects/pypads-notebooks/venv/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "train = np.append(X_train, y_train.reshape((y_train.shape[0],1)), axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Any\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def _forward_unimplemented(self, *input: Any) -> None:\n",
    "        pass\n",
    "    \n",
    "    def __init__(self, feature_shape, leaky_value=0.05, shape=[30, 50, 1]):\n",
    "        super(Model, self).__init__()\n",
    "        layers = [feature_shape] + shape\n",
    "        layer_list = OrderedDict()\n",
    "        for idx in range(1, len(layers)):\n",
    "            layer_list[str(idx)] = nn.Linear(layers[idx-1], layers[idx])\n",
    "            layer_list['Relu' + str(idx)] = nn.LeakyReLU(leaky_value, inplace=True)\n",
    "\n",
    "        layer_list['Sigmoid'] = nn.Sigmoid()\n",
    "        self.model = nn.Sequential(layer_list)\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.shape[1]\n",
    "num_labels = 3\n",
    "num_epochs = 75\n",
    "test_every = 10\n",
    "h_lr = 0.005\n",
    "h_b1 = 0.5\n",
    "h_b2 = 0.999\n",
    "batch_size = 10\n",
    "neural_network_shape = [10, 10, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(feature_shape=num_features, shape=neural_network_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=h_lr, betas=(h_b1, h_b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_one_epoch(model, loader: DataLoader, tensor, optimizer, loss_, scheduler=None):\n",
    "    total_loss = 0\n",
    "    n_iter = 0\n",
    "    for batch_idx, (batch_train_features, batch_train_labels) in enumerate(loader):\n",
    "\n",
    "        features_batch = tensor(batch_train_features.type(torch.FloatTensor))\n",
    "        labels_batch = tensor(batch_train_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        forward = model.forward(features_batch)\n",
    "        _loss = loss_(forward, labels_batch)\n",
    "\n",
    "        _loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += _loss\n",
    "        n_iter += 1\n",
    "\n",
    "    return total_loss / n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_collate(batch):\n",
    "    one_hot = True\n",
    "    num_labels = 3\n",
    "    X = torch.stack([torch.tensor(x[:-1]) for x in batch])\n",
    "    if one_hot is False:\n",
    "        Y = [y[-1] for y in batch]\n",
    "    else:\n",
    "        Y = torch.zeros(len(batch), num_labels)\n",
    "        for idx in range(len(batch)):\n",
    "            Y[idx][int(batch[idx][-1])] = 1\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train, batch_size=10, shuffle=True, collate_fn=sample_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy is 0.633333 \n",
      "Average loss during epoch 0: 0.682202 with execution time: 0.01\n",
      "Average loss during epoch 1: 0.675866 with execution time: 0.01\n",
      "Average loss during epoch 2: 0.668212 with execution time: 0.01\n",
      "Average loss during epoch 3: 0.659434 with execution time: 0.01\n",
      "Average loss during epoch 4: 0.648030 with execution time: 0.01\n",
      "Average loss during epoch 5: 0.633524 with execution time: 0.01\n",
      "Average loss during epoch 6: 0.615018 with execution time: 0.01\n",
      "Average loss during epoch 7: 0.592594 with execution time: 0.01\n",
      "Average loss during epoch 8: 0.537318 with execution time: 0.01\n",
      "Average loss during epoch 9: 0.516329 with execution time: 0.01\n",
      "Accuracy at 10 epoch is 0.650000 \n",
      "Average loss during epoch 10: 0.505683 with execution time: 0.01\n",
      "Average loss during epoch 11: 0.483489 with execution time: 0.01\n",
      "Average loss during epoch 12: 0.473819 with execution time: 0.01\n",
      "Average loss during epoch 13: 0.462664 with execution time: 0.01\n",
      "Average loss during epoch 14: 0.447927 with execution time: 0.01\n",
      "Average loss during epoch 15: 0.431787 with execution time: 0.01\n",
      "Average loss during epoch 16: 0.416558 with execution time: 0.01\n",
      "Average loss during epoch 17: 0.423921 with execution time: 0.01\n",
      "Average loss during epoch 18: 0.411870 with execution time: 0.01\n",
      "Average loss during epoch 19: 0.384897 with execution time: 0.01\n",
      "Accuracy at 20 epoch is 0.650000 \n",
      "Average loss during epoch 20: 0.384347 with execution time: 0.01\n",
      "Average loss during epoch 21: 0.382977 with execution time: 0.01\n",
      "Average loss during epoch 22: 0.367555 with execution time: 0.01\n",
      "Average loss during epoch 23: 0.379936 with execution time: 0.01\n",
      "Average loss during epoch 24: 0.350976 with execution time: 0.01\n",
      "Average loss during epoch 25: 0.350120 with execution time: 0.01\n",
      "Average loss during epoch 26: 0.348260 with execution time: 0.01\n",
      "Average loss during epoch 27: 0.349278 with execution time: 0.01\n",
      "Average loss during epoch 28: 0.333166 with execution time: 0.01\n",
      "Average loss during epoch 29: 0.331310 with execution time: 0.01\n",
      "Accuracy at 30 epoch is 0.650000 \n",
      "Average loss during epoch 30: 0.334108 with execution time: 0.01\n",
      "Average loss during epoch 31: 0.322480 with execution time: 0.01\n",
      "Average loss during epoch 32: 0.312741 with execution time: 0.01\n",
      "Average loss during epoch 33: 0.326355 with execution time: 0.01\n",
      "Average loss during epoch 34: 0.329043 with execution time: 0.01\n",
      "Average loss during epoch 35: 0.316547 with execution time: 0.01\n",
      "Average loss during epoch 36: 0.317304 with execution time: 0.01\n",
      "Average loss during epoch 37: 0.310586 with execution time: 0.01\n",
      "Average loss during epoch 38: 0.296103 with execution time: 0.01\n",
      "Average loss during epoch 39: 0.298967 with execution time: 0.01\n",
      "Accuracy at 40 epoch is 0.666667 \n",
      "Average loss during epoch 40: 0.299661 with execution time: 0.01\n",
      "Average loss during epoch 41: 0.302193 with execution time: 0.01\n",
      "Average loss during epoch 42: 0.288491 with execution time: 0.01\n",
      "Average loss during epoch 43: 0.301859 with execution time: 0.01\n",
      "Average loss during epoch 44: 0.304358 with execution time: 0.01\n",
      "Average loss during epoch 45: 0.281517 with execution time: 0.01\n",
      "Average loss during epoch 46: 0.293113 with execution time: 0.01\n",
      "Average loss during epoch 47: 0.279373 with execution time: 0.01\n",
      "Average loss during epoch 48: 0.294310 with execution time: 0.01\n",
      "Average loss during epoch 49: 0.295048 with execution time: 0.01\n",
      "Accuracy at 50 epoch is 0.750000 \n",
      "Average loss during epoch 50: 0.287552 with execution time: 0.01\n",
      "Average loss during epoch 51: 0.280541 with execution time: 0.01\n",
      "Average loss during epoch 52: 0.285465 with execution time: 0.01\n",
      "Average loss during epoch 53: 0.281586 with execution time: 0.01\n",
      "Average loss during epoch 54: 0.281233 with execution time: 0.01\n",
      "Average loss during epoch 55: 0.281929 with execution time: 0.01\n",
      "Average loss during epoch 56: 0.280396 with execution time: 0.01\n",
      "Average loss during epoch 57: 0.275505 with execution time: 0.01\n",
      "Average loss during epoch 58: 0.271287 with execution time: 0.01\n",
      "Average loss during epoch 59: 0.289972 with execution time: 0.01\n",
      "Accuracy at 60 epoch is 0.933333 \n",
      "Average loss during epoch 60: 0.288080 with execution time: 0.01\n",
      "Average loss during epoch 61: 0.267712 with execution time: 0.01\n",
      "Average loss during epoch 62: 0.264819 with execution time: 0.01\n",
      "Average loss during epoch 63: 0.285846 with execution time: 0.01\n",
      "Average loss during epoch 64: 0.266947 with execution time: 0.01\n",
      "Average loss during epoch 65: 0.281895 with execution time: 0.01\n",
      "Average loss during epoch 66: 0.265253 with execution time: 0.01\n",
      "Average loss during epoch 67: 0.274912 with execution time: 0.01\n",
      "Average loss during epoch 68: 0.255481 with execution time: 0.01\n",
      "Average loss during epoch 69: 0.258187 with execution time: 0.01\n",
      "Accuracy at 70 epoch is 0.966667 \n",
      "Average loss during epoch 70: 0.259931 with execution time: 0.01\n",
      "Average loss during epoch 71: 0.265738 with execution time: 0.01\n",
      "Average loss during epoch 72: 0.253350 with execution time: 0.01\n",
      "Average loss during epoch 73: 0.282969 with execution time: 0.01\n",
      "Average loss during epoch 74: 0.259133 with execution time: 0.01\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "labels = [i for i in range(num_labels)]\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "tensor = torch.cuda.FloatStorage if cuda else torch.FloatTensor\n",
    "\n",
    "results = model.forward(tensor(X_test))\n",
    "predicted = torch.max(results.data, 1)\n",
    "y_pred = predicted.indices.tolist()\n",
    "print('Initial accuracy is %f ' % accuracy_score(y_true=y_test.tolist(), y_pred=y_pred))\n",
    "\n",
    "epochs_trained = 0\n",
    "\n",
    "for idx in range(num_epochs):\n",
    "\n",
    "    t1 = time()\n",
    "    n_iter = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    total_loss = train_one_epoch(model=model, loader=loader, optimizer=optimizer, loss_=loss_, tensor=tensor)\n",
    "\n",
    "    t2 = time()\n",
    "    print('Average loss during epoch %d: %f with execution time: %0.2f' % (idx, total_loss, (t2 - t1)))\n",
    "    \n",
    "    epochs_trained += 1\n",
    "    if epochs_trained % test_every == 0:\n",
    "        results = model.forward(tensor(X_test))\n",
    "        predicted = torch.max(results.data, 1)\n",
    "        y_pred = predicted.indices.tolist()\n",
    "        print('Accuracy at %d epoch is %f ' % (epochs_trained, accuracy_score(y_true=y_test.tolist(), y_pred=y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-05 14:37:15.890 | WARNING  | pypads.injections.analysis.parameters:__post__:85 - No parameters are defined on the mapping file for <class 'sklearn.preprocessing._label.LabelEncoder'>. Trying to extract by other means...\n",
      "2020-10-05 14:37:16.626 | WARNING  | pypads.injections.analysis.parameters:__post__:85 - No parameters are defined on the mapping file for <class 'sklearn.preprocessing._label.LabelEncoder'>. Trying to extract by other means...\n",
      "2020-10-05 14:37:17.358 | WARNING  | pypads.injections.analysis.parameters:__post__:85 - No parameters are defined on the mapping file for <class 'sklearn.preprocessing._label.LabelEncoder'>. Trying to extract by other means...\n",
      "2020-10-05 14:37:18.094 | WARNING  | pypads.injections.analysis.parameters:__post__:85 - No parameters are defined on the mapping file for <class 'sklearn.preprocessing._label.LabelEncoder'>. Trying to extract by other means...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.86      0.92        21\n",
      "           2       0.87      1.00      0.93        20\n",
      "\n",
      "    accuracy                           0.95        60\n",
      "   macro avg       0.96      0.95      0.95        60\n",
      "weighted avg       0.96      0.95      0.95        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = model.forward(tensor(X_test))\n",
    "predicted = torch.max(results.data, 1)\n",
    "y_pred = predicted.indices.tolist()\n",
    "print(classification_report(y_true=y_test.tolist(), y_pred=y_pred, labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
